# -*- coding: utf-8 -*-
"""term-deposit-marketing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tdewGu3dhcgpWTpNU3yNWQ1TVE6sZe9S
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

df = pd.read_csv("/content/term-deposit-marketing-2020.csv")
df.columns

df.isna().sum().sum()

df.shape

df.dtypes

target = {'yes':1, 'no':0}
df.y.replace(target, inplace=True)

df.head()

"""# Exploratory Data Analysis"""

df.y.value_counts() #imbalanced data

#Age
fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10,5))
ax1.hist(df[df.y == 1].age)
ax1.set_title("Age Dist of Subscribed")
ax2.hist(df[df.y == 0].age)
ax2.set_title("Age Dist of Not Subscribed")

df[['age','y']].groupby('y').mean()

#Age and marital status
df[['age','marital','y']].groupby(['y','marital']).mean()

#Job
df[['job','y']].groupby('job').agg(['mean','count','sum']).sort_values(by=('y','mean'), ascending=False)

df.job.replace(['self-employed','admin','technician','unknown'], 'other', inplace=True)

#Marital
df[['marital','y']].groupby('marital').agg(['mean','count','sum'])

#Education
df[['education','y']].groupby('education').agg(['mean','count','sum'])

df.education.replace(['unknown', 'primary', 'secondary'], 'other', inplace=True)

#Duration (very important)
df[['y','duration']].groupby('y').mean()

df[df.duration > 1000].y.value_counts()

df[df.duration <= 1000].y.value_counts()

df.duration.max()

fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10,5))
ax1.hist(df.age, range=(0, 1000))
ax1.set_title("Call duration (0-1000 seconds)", fontsize=14)
ax2.hist(df[df.y == 1].duration, range=(0,1000))
ax2.set_title("Call duration of subscribed (0-1000)", fontsize=14)

#Default (may be excluded)
df[['default','y']].groupby('default').agg(['mean','count'])

#Balance (important)
df[['y','balance']].groupby('y').mean()

#Housing
df[['housing','y']].groupby('housing').agg(['mean','count'])

#Loan
df[['loan','y']].groupby('loan').agg(['mean','count'])

#Month
df[['month','y']].groupby('month').agg(['mean','count','sum']).sort_values(by=('y','mean'))

corr = df.corr()
plt.figure(figsize=(10,6))
sns.heatmap(corr, annot=True)

df = df.drop(['day','default','month'], axis=1)
df.head()

df = pd.get_dummies(df, drop_first=True)
df.head()

"""## Model"""

from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

X = df.drop('y', axis=1)
y = df['y']

#train test split - logistic regression
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
sc = MinMaxScaler().fit(X_train)
X_train = sc.transform(X_train)
X_test = sc.transform(X_test)
lr = LogisticRegression(class_weight={0:1, 1:3})
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

#cross validation - logistic regression
lr2 = LogisticRegression(class_weight={0:1, 1:3})
clf = make_pipeline(MinMaxScaler(), lr2)
scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
print(scores.mean())

#cross validation - random forest
rf = RandomForestClassifier(n_estimators=150, max_depth=6, class_weight={0:1, 1:3})
scores2 = cross_val_score(rf, X, y, cv=5, scoring='accuracy')
print(scores2.mean())

"""Logistic regression achieved 5-fold cross validation score of 92.3%. The average score turns out to be 91.8% with random forest. 

Since there is an imbalance between the classes in the target variable (y), I have adjusted the class weight so that the model does not focus too much on the dominant class.

Depending on the customer needs, the model can be trained to optimize different metric such as precision and recall. For instance, if customer demands to detect all of the positive classes (i.e. y=1), recall should be maximixed.

# Bonus section

## What makes the customers buy?
"""

rf = RandomForestClassifier(n_estimators=150, max_depth=6).fit(X_train, y_train)
rf.feature_importances_.round(3)

df.head()

"""The most significant feature is "duration". The longer the call, the more likely that a customer subscribes. We also see that in the exploratory data analysis."""

df[df.duration <= 200].y.value_counts()

df[df.duration > 200].y.value_counts()

df[['duration','y']].groupby('y').mean()

"""The averate duration of subscribed customers is 682 seconds whereas it is 221 seconds for customers who did not subscribe.

The calls that are longer than 200 seconds are much more likely to make a customer to subscribe than the calls shorter than 200 seconds.
"""